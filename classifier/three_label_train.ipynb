{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9435bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI Classifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'dataset'\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "RANDOM_SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d3a877c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning: C:\\Users\\HP\\Desktop\\classification model\\dataset\n"
     ]
    }
   ],
   "source": [
    "data_dir = pathlib.Path(DATA_DIR).resolve()\n",
    "print(f\"Scanning: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1cce111d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: ['animals', 'humans', 'other']\n"
     ]
    }
   ],
   "source": [
    "class_names = sorted([item.name for item in data_dir.glob('*') if item.is_dir()])\n",
    "class_indices = {name: index for index, name in enumerate(class_names)}\n",
    "print(f\"Classes found: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28f7163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp','*.webp','*.jfif','*.tiff']\n",
    "all_image_paths = []\n",
    "all_image_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2743a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for images...\n",
      " Found 1799 images total.\n"
     ]
    }
   ],
   "source": [
    "print(\"Searching for images...\")\n",
    "for ext in extensions:\n",
    "    for path in data_dir.rglob(ext):\n",
    "        path_obj = pathlib.Path(path)\n",
    "        try:\n",
    "            relative_path = path_obj.relative_to(data_dir)\n",
    "            main_class = relative_path.parts[0]\n",
    "            if main_class in class_indices:\n",
    "                all_image_paths.append(str(path))\n",
    "                all_image_labels.append(class_indices[main_class])\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "print(f\" Found {len(all_image_paths)} images total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41fe207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    all_image_paths, all_image_labels, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_SEED, \n",
    "    stratify=all_image_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48f1c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_pad_image(path, label):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize_with_pad(img, IMG_SIZE, IMG_SIZE)\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    label = tf.one_hot(label, depth=len(class_names))\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee11988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(paths, labels, is_training=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    ds = ds.map(load_and_pad_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if is_training:\n",
    "        ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "abf7df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_dataset(train_paths, train_labels, is_training=True)\n",
    "val_ds = create_dataset(val_paths, val_labels, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c83406",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),      \n",
    "], name=\"augmentation_block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c312565",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = data_augmentation(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3f317708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12920\\2933382203.py:1: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=x)\n"
     ]
    }
   ],
   "source": [
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=x)\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85284017",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "001c7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Dense(128)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbfa0eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = layers.Dense(len(class_names), activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b5dafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f6cf7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e7fec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training (Frozen Base)...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting Training (Frozen Base)...\")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0304129b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 2s/step - accuracy: 0.8353 - loss: 0.3437 - val_accuracy: 0.9194 - val_loss: 0.2065\n",
      "Epoch 2/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - accuracy: 0.9527 - loss: 0.1390 - val_accuracy: 0.9556 - val_loss: 0.1345\n",
      "Epoch 3/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - accuracy: 0.9715 - loss: 0.0953 - val_accuracy: 0.9667 - val_loss: 0.0815\n",
      "Epoch 4/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 0.9729 - loss: 0.0793 - val_accuracy: 0.9722 - val_loss: 0.0701\n",
      "Epoch 5/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 2s/step - accuracy: 0.9701 - loss: 0.0699 - val_accuracy: 0.9556 - val_loss: 0.0740\n",
      "Epoch 6/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 2s/step - accuracy: 0.9778 - loss: 0.0603 - val_accuracy: 0.9778 - val_loss: 0.0623\n",
      "Epoch 7/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 2s/step - accuracy: 0.9826 - loss: 0.0472 - val_accuracy: 0.9722 - val_loss: 0.0668\n",
      "Epoch 8/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9798 - loss: 0.0502 - val_accuracy: 0.9722 - val_loss: 0.0581\n",
      "Epoch 9/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 2s/step - accuracy: 0.9868 - loss: 0.0413 - val_accuracy: 0.9778 - val_loss: 0.0573\n",
      "Epoch 10/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 2s/step - accuracy: 0.9861 - loss: 0.0359 - val_accuracy: 0.9778 - val_loss: 0.0560\n",
      "Epoch 11/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9833 - loss: 0.0400 - val_accuracy: 0.9694 - val_loss: 0.0684\n",
      "Epoch 12/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - accuracy: 0.9847 - loss: 0.0388 - val_accuracy: 0.9694 - val_loss: 0.0688\n",
      "Epoch 13/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - accuracy: 0.9847 - loss: 0.0333 - val_accuracy: 0.9806 - val_loss: 0.0580\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=20, \n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7de1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"sigmoid_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825bc981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
